[TOC]

# 一、并行平台

## 并行硬件

### Flynn分类法

![1.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/1.png?raw=true) 

### SIMD（单指令流多控制流）（GPU）

单一控制部件向每个处理部件分派指令

*条件执行会降低SIMD处理器的性能，会产生操作屏蔽码，需要小心。

![2.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/2.png?raw=true) 

### MIMD（多指令流多控制流）

每个处理器可以独立于其他处理器执行不同的程序

![3.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/3.png?raw=true) 

### 通信模型

#### 共享地址空间平台（Shared Address System）

平台支持一个公共的数据空间，所有处理器都能访问该空间。处理器通过修改在共享地址空间的数据来实现交互。

支持SPMD编程的共享地址空间平台也称为**多处理器**（multiprocessors）

共享地址空间的内存既可以是本地的（处理器独占），也可以是全局的（对所有处理器共用）

##### 一致内存访问（UMA=SMP）

处理器访问系统中的任何内存（不含高速缓存）字的时间都相同

所有的处理器都连接到一个相同的共享内存当中，操作简单，不用划分数据

但是由于处理器增多时，会产生内存方面的竞争，所以带宽（bandwidth）有限，可扩展性差（scaling），只用于有限量的处理器

![4.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/4.png?raw=true) 

##### 非一致内存访问（NUMA）

内存共享，但处理器访问系统中的任何内存（不含高速缓存）字的时间不都相同

这样可以解决scaling的问题

但是，这个问题不能解决内存访问由于距离过长而导致访存时间太长的问题，所以每个处理器有一个自己的cache

所以NUMA又叫做cache­-coherent nonuniform memory access systems（ccNUMA） 

![5.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/5.png?raw=true) 

#### 分布式内存系统（Distributed Memory）

##### 消息传递平台（Message Passing）

每个线程有自己的独立地址空间，交互是通过发送和接收消息完成的（send，receive）

![6.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/6.png?raw=true) 

##### MPP (massively parallel processors)  

#####集群(cluster)

最广泛使用的分布式内存系统称为**集群(cluster)**。现成的网络和现成的电脑相连。

#### 混合系统（Hybrid Distributed-Shared Memory）

目前大的和快的计算机都在用这种系统，每个节点都有自己的共享内存系统。

里面可以既是CPU，也可以是GPU

![7.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/7.png?raw=true)  

#### 网格（grids）

网格是一种基础架构，使地理上分布的大型网络转换成一个分布式内存系统。

#### 数据并行（Data Parallel）

## 并行程序设计

### 增强并行度（Incremental parallelization）

1. 理解串行程序
2. 找到程序的瓶颈位置或者有机会并行的地方
3. 尽量让所有内核都在忙于工作

###Foster 方法（Foster’s Design Methodology）

![8.png](https://github.com/wxmkevin/PKU/blob/master/parallel%20programming/screenshots/8.png?raw=true) 

#### 划分（partitioning）

把问题分成好几个任务

#### 通信（communication）

确定上一步所识别出来的任务之间存在哪些通信

#### 凝聚或聚合（agglomeration or aggregation）

将第一步所确定的任务与通信结合成更大的任务。例如，如果任务A必须在任务B之前进行，那么应该将这两个任务聚合成一个简单的大任务

####分配（mapping）

将上一个分配好的任务分配到进程/线程中。这一步尽量减少通讯量，并且尽量使每一个进程/线程所得到的工作量大致平衡

## 并行度（Degree of Parallelism）

**并行度**：可以并行运算的任务的个数

可以用最大和平均来计算。

**任务粒度**（task granularity）和**并行度**负相关。

**关键路径**（critical path）：任务关系图中最长的路径。它的长度是运算的下界。

### 并行度的表现

加速比（speedup）：$T_1/T_p$

并行效率（parallel efficiency）：$T_1/pT_p$

### 阿姆达定律（Amdahl‘s Law）

$$
speedup=\frac{1}{1-p+\frac{p}{n}}
$$

p为并行程序所占时间比，n为处理器个数



# 二、OpenMP

## Fork-Join Model

Fork：分配线程

Join：将线程终结，归入到主线程

## API

1. Compiler Directives：编译命令
2. Runtime Library Machines：运行库程序 
3. Environment Variables：环境变量

## 语法

### 基本框架

`#include<omp.h>`

`#pragma`：由预处理程序处理，无法识别也可以继续跑

`#pragma omp <directive-name>[clause,...]`

每个指令最多只适用于一个后续的声明，而且该后续声明必须是一个结构化的块。

在之领航的末尾通过使用反斜杠“\”转移换行符，可以在后续行上“延续”长指令行。 

### 函数

`int omp_get_num_procs(void)`：返回可以用的处理器数目

`void omp_set_num_threads(int t)`：选择线程数目

`int omp_get_thread_num()`：获取当前线程编号

### 锁（lock）

`omp_lock_t`锁的数据结构类型，不同变量名的锁不同

`void omp_init_lock(omp_lock_t* lock_p)`初始化锁

`void omp_set_lock(omp_lock_t* lock_p)`上锁（如果临界区之前没有上锁，则该线程上锁并进入临界区；否则等待）

`void omp_unset_lock(omp_lock_t* lock_p)`解锁

`void omp_destroy_lock(omp_lock_t* lock_p)`销毁锁

### directive-name

一个合法的OpenMP指令。需要出现在pragma之后，所有其它从句之前。 

#### parallel

用于表示一整块的代码都应当并行计算（SPMD模式）

#### for

必须在一个并行模式中的代码中使用

for循环的任务被分配给好几个线程中

在for的结尾会有barrier，只有for循环都执行完毕才会继续执行之后的代码。

#### parallel for

告诉程序立刻在之后的for循环中并行计算

for循环不能包含：（1）break, return,exit等退出语句；（2）goto等跑到循环外的语句

每个线程都被分配了独立的循环任务集合

#### single

必须在一个并行模式中的代码中使用

只有一个线程处理该段语句

在I/O或者其他线程不能并行的情况

其他线程在single段执行结束前，会停下来等single段（除非用nowait）

#### master

主线程执行的语句

其他线程不会等

#### sections，section

sections声明之后包含的section需要并行（被分配给不同进程）

独立的section指令都在sections中的命令下执行

每个section只被执行一次，不同的section有可能被不同的线程执行，一个线程有可能执行多个section

#### num_threads(int)

声明有多少个线程执行任务

#### task

用于非典型的问题的并行运算：例如没有界的循环，递归问题等

task可以延期执行，也可以立即执行，这些由runtime system决定

同步方法：

1. `#pragma omp barrier`所有线程同步
2. `#pragma omp taskwait`由任务产生的子线程同步

#### barrier

`#pragma omp barrier`

barrier前每个线程都完成后才能开始barrier后的任务

有些语句是有隐式barrier，例如for，single等，这些可以用nowait去除

#### critical    

`#pragma omp critical[name]`

该段代码（name相同的部分）只能一个线程执行

要用critical语句保护所有公有资源

name不同的critical部分是可以同时执行的

没有写name的critical部分按名字相同处理

#### atomic

`#pragma omp atomic`

只能保护一条C语言赋值语句所形成的临界区。

语句必须是以下形式之一：

```
x <op>= <expression>;
x++;
++x;
x--;
--x;
```

expression不含x，并且只有x装载和存储可以确保是受保护的

#### critical vs atomic

atomic没有命名，所以需要多个保护临界区的话，用命名的critical或者lock

critical和atomic不互斥

#### flush

`#pragma omp flush (<variable-list>)`

立刻刷新variable list中的所有公有变量

**所有的隐式flush**：

barrier 

parallel - upon entry and exit 

critical - upon entry and exit 

ordered - upon entry and exit （ordered：按顺序执行循环）

for - upon exit 

sections - upon exit 

single - upon exit    

### clause（子句）

可选。从句可以以任意次序出现，并且可以在需要的时候重复出现出现（特殊情况除外）。 

#### private    

`pragma omp … private (<variable list>)`

声明一个变量为私有变量，那么每一个线程都不能访问其他的私有变量，也不能访问或更改公有变量。

#### schedule（循环调度）

在缺省情况下parallel for 的任务分配是等额（循环次数相同）的

##### A. static schedule

`schedule (static [, chunk])    `

在循环执行前分配

chunk代表一次分配的块（循环数）的大小：chunk没有的时候和没写static是一样的

（例：chunk=2: threads1:0,1; threads2: 2,3）

##### B. dynamic schedule

`schedule (dynamic [, chunk])    `

在循环执行后分配

线程完成一块会请求另一块，chunk为分配一块的大小，默认时为1

可以解决循环大小不一样的问题

##### C. guided schedule

`schedule (guided [, chunk])    `

开始分配大的块，之后每次分配的越来越小，但大小不能小于chunk，chunk默认值为1
$$
guided\_block\_size=constant\cdot\frac{remain\_blocks}{number\_of\_threads}
$$
constant近似为1

#### reduction（归约）

适用于parallel，sections和for后

OpenMP为每个线程有效地创建了一个私有变量，运行时系统在这个私有变量中存储每个线程的结果。

OpenMP也创建了临界区，并且在这个临界区中，将存储在私有变量的值op操作。

`#pragma omp … reduction (op : list)`

私有变量被复制，初始值取决于op

| op   | 初始值 |
| ---- | ------ |
| +    | 0      |
| *    | 1      |
| -    | 0      |
| ^    | 0      |
| &    | ~0     |
| \|   | 0      |
| \|\| | 1      |
| &&   | 0      |

有些是不满足交换律的，需要注意！

#### nowait

适用于去掉for，single的barrier

## 同步（synchronization）问题

**同步**：一种使得读写能按照合理顺序运行的一种机制（无论线程是如何安排的）

### barrier

设置barrier进行等待

###mutual exclusion

每个时间段只允许一个线程访问共有资源

####race condition

1. 有时候没问题，有时候有问题
2. 小数据一般没有问题，大数据容易出错
3. 线程增多时容易出错
4. 比较难debug

解决race condition的方法：

1. 设置私有变量：OpenMP（private子句），变量在线程内部定义
2. critical section：

#### A. critical section

在critical section中，每段时间只允许一个线程访问该部分

####B. 锁（lock）

注意锁住的是数据，不是代码



# 三、并行算法设计法则

##基本知识

###分解（decomposition）、任务（task）和依赖图（task dependency graph）

把一个计算分为很多小的部分，其中的一些或所有部分都可能被并行执行，该划分过程称为**分解（decomposition）**

**任务（task）**是程序员定义的计算单元，其中为主要计算通过分解得到的划分。

有一些任务可能需要使用别的任务所产生的数据，这样就要等到这些数据产生后再执行。**任务依赖图（task dependency graph）**表示**任务间依赖**关系和任务的**执行次序**的关系的图。

### 粒度（granularity）、并发性（concurrency）和任务交互（task interaction）

分解问题得到的任务数量和大小决定了分解的**粒度（granularity）**

将任务分解成大量的细小任务称为**细粒度（fine-grained）**

将任务分解成少量的大任务称为**粗粒度（coarse-grained）**

**并发度（degree of concurrency）**是与粒度相关的概念。并行程序中，任意时刻可同时执行的最大任务数为**最大并发度**。**平均并发度**是个更加有效的指标，它是程序执行的整个过程中能并发运行的任务平均数。这两个指标与任务粒度的大小负相关。

并发度也依赖于任务依赖图的形状，通常，同样的粒度并不能确保同样的并发度。

**关键路径（ critical path ）**是任务依赖固的另一个特性，它决定一个给定粒度的任务依赖图的平均并发度。

在一个任务依赖图中，我们把没有输入边的节点称为起始节点，把没有输出边的节点称为终止节点，任何一对起始节点和终止节点之间的最长有向路径就是关键路径。关键路径上所有节点的权之和称为关键路径长度，其中，节点的权是相应任务有关的工作量或任务的大小。**总工作量与关键路径长度的比就是平均并发度**。因此较短的关键路径有利于达到较高的并发度。

虽然通过增加分解的性度和利用所产生的并发度似乎可以并行执行越来越多的任务，以减少求解一个问题所需的时间，但实际上并非总是如此。通常对于某一问题，都有它自己固有的细粒度分解限度。

除了有限的粒度和并发度之外，还有一个重要因素，使我们不能从并行化在得不受限的加速比（串行执行时间与并行执行时间的比率）．此因素是运行在不同物理处理器上的任务之间的**交互（ interaction ）**。

**问题分解得到的任务通常要共享输入、输出或者中间数据。**任务依赖图的依赖性通常源自一个事实：某个任务的输出是另外一个任务的输入。

任务之间的交互方式通过所谓的**任务交互图（ task-interaction graph ）**来描述。任务交直固中的节点代表任务，边连接彼此交互的任务。

## 分解技术（decomposition techniques）

### 数据分解（Data Decomposition）

#### 步骤

1. 对计算中的数据进行划分
2. 在数据划分的基础上推导从计算到任务的划分

#### 划分输入数据

当输出可由输入的一个函数计算得出时可以应用。

输出未知时的唯一分解方法。

#### 划分输出数据

当输出数据可以独立计算时可以应用

#### 划分中间结果数据

有些算法通常可以组成多级计算结构，其中某一级的输出是下一级输入。对这种算法的分解。可以从划分算法中间级的输入或输出数据导出． 划分中间数据有时比划分输入或输出数据获得更高的并发性。通常， 在求解问题的串行算法中并不显式产生中间数据，而某些对原始算捷的重构可能需要用中间数据划分导出分解。

#### 划分输入和输出

在可能对输出数据进行划分的情况下，对辘入数据的划分可能导致附加的并发性。

####拥有者－计算规则（ owner-compute rule）

以划分输入或输出数据为基础的分解也常称为拥有者－计算（ owner-compute）规则。这个规则的思想是． 每一个划分都执行涉及它拥有的数据的所有计算。根据数据的性质或数据划分的类型，拥有者-计算规则可能具有不同的含义。

### 递归分解（recursive decomposition）

递归分解采用分治策略使问题可并行执行．这种策略首先划分问题为一组独立的子问题，子问题又可以采用相似的划分得到更小的子问题．分治策略能得到自然的并发性，因为不同子问题可并发求解．

### 探测性分解（exploratory decomposition）

有些问题的基本计算对应于解空间的一次搜索，**探测性分解**就用来分解这样的问题。在探测性分解中、划分搜索空间为更小的部分，然后井发搜索这些小的部分，直至找出希望的解。

即使探测性分解表面上与数据分解相似（搜索空间可认为是被划分的数据），但它们有下述本质上的不同：

数据分解导出的任务都完全地执行，每一任务执行的计算都是最终解的一部分。在另一方面， 探测性分解中只要一个任务找到答案，其他未完成任务就可以终止． 因此，并行形式执行的一部分使索（ 以及执行的累计操作量） 与串行算法的搜索是完全不同的。并行形式执行的操作既可以少于也可以多于串行算法执行的操作。

### 推测性分解（speculative decomposition） 

有时程序会遇到多个可能的对计算很重要的分支，逸事事某个分支又取决于之前的其他计算的结果， 此时就要用到**推测性分解**。

推测性分解与探测性分解有如下不同：推测性分解中导向各并行任务的一个分支输入是未知的．而在探测性分解中．源于一个分支的多任务输出是未知的。在推测性分解情况下，串行算法严格地执行一个推测阶段的单个任务，因为当到达此阶段的开始处时．已经确切知道应该执行哪一个分支．因此， 通过预先计算只有一个实际被执行的多个可能的任务．采用推测性分解的并行要比相应的串行程序完成更多的累积工作．即使只是推测性地探测多个可能性中的一个

# 四、MPI

## 消息传递模型（Message-Passing Programming）

### 消息传递模型

逻辑上把支持消息传递模式的计算机视为含有p个进程，每个进程都有独占的地址空间。

1. 每一数据单元必须属于空间的分块之一；因此，数据必须被显式地划分和存放。这会使编程更复杂，但能促进存取本地化，这一点对于在非UMA结构上获得高性能是至关重要的，因为在这种结构中，处理器存取本地数据的速度要比存取远程数据快得多。
2. 所有的相互操作（只读或读／写）需要两个进程间的协作一一拥有数据的进程及想要存取数据的进程． 这种对协作的要求由于多种原因使编程变得更复杂．拥有数据的进程必须参与相互操作．哪怕它与请求进程的事件没有逻辑联系．

###消息传递程序的结构

人们通常使用**异步 ( asynchronous ）**或**松散同步（ loosely synchronous ）**模式来编写消息传递程序。在异步模式中，所有并发的任务都异步执行。这使得实现任何并行算法成为可能。然而，这样的程序很难理解，并且由于存在竞争条件可能导致不确定的行为。松散同步程序是这两个极端阔的很好的折衷方案。在这种程序中，任务或任务的子集同步执行交互。然而，在这些交互之间，任务的执行是完全异步的。由于交互同步发生，很容易理解程序，许多已知的并行算法能够很自然地用松散同步程序来实现。

从最一般的形式来看，消息传递模式支持在p个进程的每个进程中执行一个不同的程序。这样就给井行编程带来非常大的灵活性，但也造成编写并行程序的工作难以扩展。因此，绝大多数消息传递程序使用**单程序多数据（ single program multiple data, SPMD ）**方法来编写。在SPMD程序中，除了在很少进程（如“根”进程）以外，不同进程中执行的代码相同。

## 发送和接收操作（Send and Receive Operations）

由于交互要通过发送及接收消息来完成，消息传递编程模式中最基本的操作就是send
（发送）和receive （接收）。采用最简化的形式时，这些操作的原型定义如下：

```c
send(void *sendbuf, int nelems, int dest)
receive(void *recvbuf, int nelems, int source)
```

sendbuf指向存储待发送数据的缓冲区， recvbuf指向存储待接收数据的缓冲区，nelems是待发送和接收的数据单元的数目， dest是接收数据进程的标识符， source是发送数据进程的标识符。

由于发送和接收操作的实现方式不同，情况未必如此．绝大多数消息传递平台都有另外的硬件来支持消息的发送和接收．它们能支持DMA （直接内存访问）和使用网络接口硬件的异步消息传输．网络接口能允许消息从内存缓冲区传到需要的位置，而不需要CPU的干预． 同样， DMA允许将某一内存位置处的数据复制到另一位置（如通讯缓冲）而不需要CPU的支持（ 一旦DMA编程完毕）。因此，如果发送操作对通铺硬件编程，并在通信操作完成前返回， 接收的信息可能是错误的。

### 阻塞式消息传递操作

一个简单的方法是让发送操作在代码语义上安全时才返回。这句话的意思不是说发送操作要等到接收方收到数据后才返回，只是表示发送操作阻塞，直到能够保证代码的语义不被破坏才返回，不管程序后面会出现什么情况。

#### 1. 无缓冲阻塞式操作（ non-buffered blocking operation ) 

发送操作在接收进程处遇到相应的撞收操作前不返回． 在这种情况下，消息发出，发送操作在通信操作完毕后返问。 通常这个过程包含发送进程和接收进程之间的握手。 发送进程发送一个与核收进程通信的请求。 当接收进程遇到了接收目标， 就会响应请求。发送进程在收到响应后启动传输操作。该操作如图6- 1 所示。因为在发送端和接收端都没有缓冲区， 这个操作也称为**无缓冲阻塞式操作（ non-buffered blocking operation )** 。（就是一直等到被接收到返回）

![9.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/9.png?raw=true) 

##### 缺点一

只有当发送和接收几乎同时发出时，阻塞式无缓冲协议才适合。然而，在异步环境中，这一点是很难预测的。空闲开销是这种协议的一个主要缺点。

##### 缺点二

下面简单的消息交换可能导撞死锁：

```C
P0:
send(&a,1,1);
receive(&b,1,1);

P1
send(&a,1,0);
receive(&b,1,0);
```

由此可以推断，在阻塞式协议中死镇是很容易出现的，必须小心地破除上述的循环等待。在上面的例子中，可以通过将某一进程中的操作顺序更换一下来解除死锁，也就是将两个进程中的发送和接收顺序反过来。 但这往往会使代码变得更复杂，且易出现错误。

####2. 阻塞式有缓冲的发送／接收（ buffered blocking operation ) 

可以利用发送端和接收端的缓冲区来解决上面提到的空闲和死锁问题。发送方有一个预先分配的缓冲用来进行消息通信。当遇到发送操作时，发送方只将数据事制到指定的缓冲区，并在复制操作完成后退回． 这时，发送方进程就能继续执行程序。因为它知道数据的任何变化都不会影响程序的语义。在上面说明的协议中，发送方和接收方都使用缓冲区， 通信操作由专用的硬件处理。

![10.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/10.png?raw=true) 

#####缺点

有缓冲发送和接收操作中的死锁虽然缓冲减少死锁的出现，但仍可能编写造成死锁的代码。这是因为，和无缓冲时的情况一样， 接收调用总是阻塞着（为了保证代码语义上的一致性）。因此，如下的简单代码段也能造成死锁，因为两个进程都等待着接收数据，但没有进程发出数据。

```C
P0:
receive(&a,1,1);
send(&b,1,1);

P1
receive(&a,1,0);
send(&b,1,0);
```

### 无阻塞式消息传递操作

####3. 无阻塞式消息传递操作

这需要用户保证语义是安全的，无阻塞式操作通常伴随着一个check-status （状态检查）操作，它显示前一个发起的传输的语义是否可能改变。当从无阻塞式发送或接收操作返回时，进程可以不受约束地执行和完成操作无关的任何计算。如果有必要的话，在程序的后面可以检查无阻塞式操作是否完成，并等待它的完成。

无阻塞式镰作也可以和有缓冲协议一起使用．在这种情况下，发送方启动一个DMA操作并立即返回. DMA操作一结束，数据就成为安全的。在接收端，接收操作启动一个从发送方的缓冲区到接收方的目标位置的传输。在无阻塞式操作中使用缓冲区可以减少数据不安全的时间。

典型的消息传递库．如消息传递接口（ MPI ）和并行虚拟机（ PVM ），既使用阻塞式操作又使用无阻塞式操作． 阻塞式操作使编程更安全，更简单，而无阻塞式操作可以通过屏蔽通信开销来进行性能优化。但是，使用无阻塞协议时一定要小心，因为不安全地存取处于通信过程中的数据会引发错误。

## MPI简介

早期，消息传递演变成现代形式的汇编语言，每个硬件制造商都提供他自己的程序库，这些库在它们自己的硬件上工作得很好。但是与其他制造商提供的并行计算机不兼容。由不同制造商提供的特定的消息传递库之间的许多差别其实只是句法上的；但是，将消息传递程序从一个库移植到另一个库中时，对一些重大的语义差别需要作很大修改。

**消息传递接口（通称为MPI ）**就是用来解决这个问题。MPI定义消息传递的标准库，这些库可以使用C或Fortran开发可移植的消息传递程序。MPI标准定义一组核心的库例行程序的语法及语义。它们对编写消息传递程序非常有用。MPI 由许多来自学术机构和企业的研究人员开发，并得到几乎所有的硬件制造商的支持。在几乎所有的商用并行计算机上都有MPI的硬件实现。

![11.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/11.png?raw=true) 

## MPI函数（例行程序 routines）

MPI库中包含超过125个例行程序，但关键的例行程序要少得多。事实上，只需要使用下表中的6个例行程序，就能写出全功能的消息传递程序．这些例行程序分别用来初始化放终止MPI库，从并行计算环境中获取信息，以及发送和接收消息

| 函数名          | 作用                 |
| --------------- | -------------------- |
| `MPI_Init`      | 初始化MPI            |
| `MPI_Finalize`  | 终止MPI              |
| `MPI_Comm_size` | 确定进程的数目       |
| `MPI_Comm_rank` | 确定选用的进程的标号 |
| `MPI_send`      | 发送一条消息         |
| `MPI_recv`      | 接收一条消息         |

### MPI_Init & MPI_Finalize

MPI_Init的调用要优先于对其他MPI例行程序的调用。它的作用是韧始化MPI环境，在程序的执行过程中对MPI_Init调用超过一次就会出错。计算结束时调用MPI_Finalize，它将执行多个收尾任务以终止MPI环境。在调用用MPI_Finalize后，不能再有其他的MPI调用， 甚至包括MPI_Init。

MPI_ Init和MPI_Finalize必须被所有的进程调用，否则MPI的状态会成为不确定的． 在C语言中，对这两个例行程序的正确调用序列如下：

```C
int MPI_Init(int *argc, char ***argv)
int MPI_Finalize ()
```

MPI_Init中的参数argc和argv是C程序中的命令行参数。MPI的实现在返回到程序前，将从argv数组中移走应在实现中处理的所有命令行参数，并相应地减小argc. 因此，对命令行的处理只有在调用MPI_Init后才能进行．在MPI_Init和MPI_Finalize成功执行后返回MPI_SUCCESS；否则返回一个由实现定义的错误代码。

对这两个函数的绑定和调用顺序表明MPI遵循通常的命名规则和参数约定。所有的MPI例行程序、数据类型以反常量都加一个前缀"MPI_”如果调用成功，则返回码是MPI_SUCCESS ，**在C语言中它和其他的MPI常量以及数据结构定义在“mpi.h”文件中．每个MPI程序必需包含这个头文件。**

###通信器、MPI_Comm

**通信域（ communication domain ）**是一个贯穿于MPI的关键概念。一个通信域是可以相互通信的一组进程的集合．有关通信域的信息存储在类型为MPI_Comm的变量中，这些变量称为**通信器（communicator）**

通信器用来定义能够相互通信的进程集合．这个进程集合构成一个通信域。通常，所有的进程都需要相互通信。因此， MPI定义一个默认的通信器MPI_COMM_WORLD ，它包含所有参与并行执行的进程。

MPI _Comm_size和MPI_Comm_rank函数分别用来确定进程的数目以反调用进程的标号。这两个例行程序的调用序列如下：

```
int MPI_Comm_size{MPI_Comm comm, int *size)
int MPI_Comm_rank(MPI_Comm comm, int *rank)
```

函数MPI_Comm_size在变量size中返回属于通信器comm进程数目。因此，如果每个处理器有一个进程，调用`MPI_Comm_size (MPI_COMM_WORLD, &size)`就会在size中返回程序用到的处理器的数目。

属于通信器的每个进程都由它的等级（rank）唯一确定．进程的等级是一个整数，其值从0到通信器的规模减1。进程可以用`MPI_Comm_rank`函数确定其在通信器中的等级， 该函数有两个参数，即通信器和一个整型变量rank. 返回时，变量rank存储进程的等级。注意调用这两个函数的每个进程都必须属于所提供的通信器，否则将会出错。

### MPI_send & MPI_recv

在MPI中，发送和接收消息的基本函数分别是MPI_Send和MPI_Recv. 这两个例行程序的调用序列如下：

```C
int MPI_Send(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)
int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm,MPI_Status *status)
```

MPI_Send发送存储在由buf指向的缓冲区中的数据，缓冲区中包含由参数datatype指定类型的连续项目。缓冲区中项目的个数由参数count给出。

下表列出MPI数据类型与C语言中给出的数据类型之间的对应。注意对于C语言中所有的数据类型，都有一个等价的MPI数据类型。但是，MPI 中有两个C语言中没有的数据类型，它们是MPI_BYTE 和MPI_PACKED

MPI_BYTE对应于一个字节（8位），而MPI_PACKED与一个数据项集合对应，这些数据项由不相邻的数据打包构成。注意MPI_Send以且其他MPI例行程序中的消息的长度**不用字节数而是以待发送的数据项的数目给出**。以数据项的数回来指定长度可以使MPI代码具有可移植性，因为在不同的体系结构中，用来存储不同的数据类型的字节数也可能不同。

由MPI_Send发出的消息的目的地由dest和comm两个参数指定。dest参数是由通信器comm指定的通信域中的目标进程的等级。每一条消息都有一个整数值的tag与之关联，它用来区分不同类型的消息。消息-tag的取值范围从0到MPI定义的常量MPI_TAG_UB. 虽然MPI_TAG_UB的值是在实现时指定．但最小为32 767。

| MPI数据类型        | C数据类型      |
| ------------------ | -------------- |
| MPI_CHAR           | char           |
| MPI_SHORT          | short          |
| MPI_INT            | int            |
| MPI_LONG           | long           |
| MPI_UNSIGNED_CHAR  | unsigned char  |
| MPI_UNSIGNED_SHORT | unsigned short |
| MPI_UNSIGNED       | unsigned int   |
| MPI_UNSIGNED_LONG  | unsigned long  |
| MPI_FLOAT          | float          |
| MPI_DOUBLE         | double         |
| MPI_LONG_DOUBLE    | long double    |
| MPI_BYTE           | -              |
| MPI_PACKED         | -              |

MPI_Recv接收到由一个进程发来的消息，该进程的等级自comm参数指定的通信域中的source给出。已发送消息的标志必须由tag参数指定。如果来自同一进程的许多消息的标志相同，那么这些消息中的任意一个被接收。MPI允许对source和tag使用通配符。如果source被设置为MPI_ANY_SOURCE ，那么通信域中的任意进程都能成为消息源。同样，如果tag被设置为MPI_ANY_TAG，那么具有任意标志的消息都被接收。接收到的消息存储在由buf指向的缓冲区中的连续单元。MPI_Recv中的count和datatype参数用来指定提供的缓冲区的长度。收到的消息应该**等于或者小于**此长度。这样就允许接收进程无需知道要发送的消息的确切大小。如果接收到的捎息的长度大于提供的缓冲的长度，就会出现溢出错误，例行程序将返回一个MPI_ERR_TRUNCATE错误。

接收消息后，可以用status 变量在取有关MPI_Recv操作的信息。在C语言中，status以MPI_Status数据结构存储。这个数据结构用含3个字段的结构实现如下：

```C
typedef struct MPI_status{
    int MPI_SOURCE;
    int MPI_TAG;
    int MPI_ERROR;
};
```

MPI_SOURCE和MPI_TAG保存接收的消息源和标志。它们在source和tag参数设置为MPI_ANY_SOURCE及MPI_ANY_TAG时尤其有用。MPI_ERROR保存接收的消息的出错码。

状态参数也能返回有关接收的消息的长度信息。此信息不能直接用status变最获得，但可以通过调用MPI _GET_count 函数获得。此函数的调用序列如下：
```C
int MPI_Get_count (MPI_Status *status, MPI_Datatype datatype,int *count)
```
MPI _Get_count中有3个参数，分别是由MPI _Recv返回的status 、datatype 中接收到的消息的类型．以及count变量中实际接收到的数据项的数目。

| send类别  | 作用                                                         |
| --------- | ------------------------------------------------------------ |
| MPI_Send  | Will not return until you can use the send buffer            |
| MPI_Bsend | Returns immediately and you can use the send buffer（ because the user must have provided buffer space with MPI_Buffer_attach） |
| MPI_Ssend | Will not return until matching receive posted                |
| MPI_Rsend | May be used ONLY if matching receive already posted          |
| MPI_Isend | Nonblocking send, but you can NOT reuse the send buffer immediately |

### MPI_Allgatherv

一个全收集通信可以连接分布在一组进程中的向量数据块，并把结果向量复制至所有的进程。

如果从每个进程收集同样数目的元素，较简单的MPI_ Allgather函数就非常合适。但在向量块分解方式中，只有当向量中的元素总数是进程总数的整数倍时，每个进程分到的向量元素才是相等的。显然这点不可能总能得到保证，所以我们使用MPl_Allgatherv。

函数声明：
```C
int MPl_Allgatherv ( void* send_buffer, int send_cnt, MPI_Datatype send_type, void* receive_buffer, int* receive_cnt, int* receive_disp, MPI_Datatype receive_type, MPI_Comm communicator)
```
除了第四个参数外其他参数都是输入参数：
send_buffer 		此进程要发送的数据的起始地址
send_cnt 		此进程要发迭的数据的个数
receive_cnt 		包含要从每个进程（包括自身〉接收的数据个数的数组
receive_disp 		从每个进程接收的数据项在缓冲区中的偏移量
receive_type 		待接收数据的数据类型
communicator 	本操作所在通信域
第四个参数， receive_buffer，是用来存放所要收集到的元素的缓冲区的起始地址。

![12.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/12.png?raw=true) 

### MPI_Scatterv

MPI Scatterv的声明：

```C
MPI Scatterv(void* send_buffer, int* send_cnt, int* send_disp, MPI_Datatype send_type, void* recv_buffer, int recv_cnt, MPI_Datatype recv_type, int root, MPI_COMM communicator)
```

本函数有9个参数，除第5个参数外全是输入变量：
• send_buffer	指向含有待分发元素缓冲区的指针。
• send_cnt		第i个元素是send_buffer 中要发送到进程i的一连串数据的个数。
• send_disp		第i个元素是send_buffer 中要发送到进程i的第一个元素在send_buffer中的偏移量。
• send_type		send_buffer中数据的类型。
• recv_buffer		指向本进程用于接收数据的缓冲区指针。
• recv_cnt		本进程要接收的数据个数。
• recv_type		recv_buffer中的数据类型。
• root			分发数据进程的ID。
• comunicator	散发操作所在的通信域。

![13.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/13.png?raw=true) 

## 编译MPI

将程序录入到文件satl .c 后，我们需要编译它。在不同的系统之间编译MPI程序的命令不同。这里是一个常见的命令行语法：
% mpicc -o satl satl.c
通过这个命令，系统将保存在satl.c中的朋程序编译并把可执行程序保存在satl中。

## 运行MPI程序

典型的运行MPI 程序的命令是mpirun。-np标记表示要生成进程的个数。我们来查看
当使用1个进程时的输出：% mpirun -np 1 satl

# 五、基本通信操作

在多数并行算法中，进程间需要交换数据。这些数据的交换常常对并行程序的效率产生重大影响，因为这些交换会在程序的执行过程中引起交互延迟。

在共享地址空阔模式的各个处理器闹，数据共事的开销可以用一个相同的表达式$t_s + mt_w$， 表示，通常对于并行计算机的不同处理器和不同的计算速度，$t_s$和$t_w$有不同的值。

我们假定互连网络支持直通路由选择。并且，任何一对节点间的通信时间与它们之间的通信路径上的中间节点数无关。同时，我们若假定通信链路是双向的；也就是说，两个直接相连的节点可以在$t_s + mt_w$时间内同时相互发送m字的消息。我们假定采用单端口通信模式，其中**一个节点一次只能在它的一条链路上发送消息**。间样， **一个节点一次只能在一条链路上接收消息**。但是，一个节点同时在同一条链路或另一条链路发送消息时可以接收消息。

## 通信成本

决定通信延迟的主要参数有以下几个：
1）启动时间（Setup time）（ $t_s$）：启动时间是在发送节点和接收节点处理消息所花费的时间．它包括消息的准备时间（添加头、尾以及错误校正信息），执行路由算法的时间，以且在本地节点和路由器之闹建立接口的时间．对于…条信息的传递来说，这种延迟只发生一次．
2 ）每站时间（Per-hop time）（$t_h$） ：当消息离开一个节点后，需要花一定的时间到达路径上的下一个节点。消息头在两个直接连接的节点间传送所花费的时间称为每站时间，也称为节点延迟( node latency ）。 每站时间与决定消息将转发到哪个输出缓冲或通道的路由选择开关直被相关。
3 ）每字传送时间（Per-word transfer time）（$t_w$）： 如果通道带宽是r个字每秒，那么每个字要花1/r秒穿过链路。这个时间称为每字传送时间，它包括网络开销以及缓冲开销。

在通常情况下，我们不看$t_h$，总开销为：
$$
t_s+mt_w
$$

## 一对多广播（one-to-all broadcast）和归约（all-to-one reduction） 

并行算法常常需要一个进程发送相同的数据给其他所有的进程或其他所有进程的子集。这种操作称为**一对多广播 ( one-to-all broadcast ）**。开始时，只有源进程具有需要广播的m字的数据。广播结束时，就会有p个原始数据的副本一一每个进程一个。一对多广播的对偶是**多对一归约（ all-to-one reduction ）** ，在多对一归约操作中， p个参与进程的每一个都有一个缓冲区M，它包含m个字．来自所有进程的数据通过一个相关的操作符组合起来，并被累加到一个目标进程中一个m字的缓冲区中。

![14.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/14.png?raw=true) 

### 环（Ring）

连续地从源进程向其他p-1个进程发送p-1个消息是一种比较简单的实现一对多广播的方法．但是，由于源进程成为瓶颈，这种方法的效率不高。而且，因为每次只从源节点和一个目标节点连接，通信网络的利用率非常低。利用一种称为**递归加倍（ recursive doubling ）**的技术，可以设计一个比较好的广播算法。递归加倍的源进程首先发送消息给另外一个进程。然后，这两个进程可以同时发送消息给还在等待消息的其他两个进程。继续这一过程，直到所有进程都收到了数据，这样消息可以在O（Iog p）步广播完毕。

![15.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/15.png?raw=true) 

广播过程

![16.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/16.png?raw=true) 

归约过程

### 格网（mesh）

我们可以把一个具有p个节点的方形格网的行或者列看作一个有$\sqrt p $个节点的线性阵列。这样，格网中的许多通信算总仅仅是一些线性阵列通信算法的简单推广。 一个线性阵列的通信操作可以在一个格网中分两个阶段来执行。第一阶段，可以将格网中的行看作是线性阵列，将操作沿着一行或所有行进行。第二阶段，再对列进行同样的操作。

考虑在有$\sqrt p$和$\sqrt p$的二维方形格网中进行的一对多广播。首先，在每一行中执行一对多广播，从源点广播到同一行的余下的$\sqrt p-1$个节点。一且格网中的一行的所有的节点都已经收到了数据．它们就在各自的列上启动一对多广播。当第二阶段结束时，格网中的每一个节点都会具有初始消息的一个副本。

![17.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/17.png?raw=true) 

### 超立方体（hypercube）

![18.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/18.png?raw=true) 

在图中，通信沿最高维开始（即由节点标号二进制表示的最高有效位确定的维），并在随着的步骤中，沿逐步降低的维数进行。在一个超立方体中，通信中选择维的顺序并不影响最后的输出结果。

### 平衡二叉树（balanced binary tree）

![19.png](https://github.com/wxmkevin/PKU_course/blob/master/parallel%20programming/screenshots/19.png?raw=true) 

### 成本分析

易知总成本为$(t_s+mt_w)log\ p$

##多对多广播（all-to-all broadcast）和归约（all-to-all reduction）

多对多广播（all-to-all broadcast）是一对多广播的推广，其中所有p个节点同时发起一个广播．虽然一个进程发送相同的m字消息给其他每个进程，但是不同的进程可以广播不同的消息。

多对多广播的对偶是多对多归约（all-to-all reduction），其中每个节点是多对一归约的目标节点。

/20

在线性阵列或者环中执行多对多广播时，在整个通信完成以前，网络中所有的通信链路可以一直保持繁忙状态， 因为每个节点与色的相邻节点之间总有某些信息需要传递，每个节点都首先把需要广播的数据发送给它的相邻节点之一。下一步，从它相邻节点之一把接收的数据转发给其他相邻节点。

## 散发和收集

